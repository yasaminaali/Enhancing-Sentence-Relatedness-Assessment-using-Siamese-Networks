# Enhancing Sentence Relatedness Assessment using Siamese Networks
NAACL2024
[Paper]()

Authors: Yasamin Aali, Sardar Hamidian, Parsa Farinneya

## Abstract
The proposed system integrates a Siamese Network architecture with pre-trained language models, including BERT, RoBERTa, and the Universal Sentence Encoder. Through rigorous experimentation and analysis, we evaluate the performance of these models across multiple languages. Our findings reveal that the Universal Sentence Encoder excels in capturing semantic similarities, outperforming BERT and RoBERTa in most scenarios. Particularly notable is the USE’s exceptional performance in English and Marathi. These results emphasize the importance of selecting appropriate pre-trained models based on linguistic considerations and task requirements.

### Embeddings
- "Universal Sentence Encoder(USE)" for [USE](https://arxiv.org/abs/1803.11175)
- "BERT" for [BERT](https://arxiv.org/abs/1810.04805)
- "RoBERTa for [RoBERTa](https://arxiv.org/abs/1907.11692)

## Cite This Paper
```
Yasamin Aali, Sardar Hamidian, and Parsa Farinneya. 2024. YSP at SemEval-2024 Task 1: Enhancing Sentence Relatedness Assessment using Siamese Networks. In Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024), pages 959–963, Mexico City, Mexico. Association for Computational Linguistics.
```
