# Enhancing Sentence Relatedness Assessment using Siamese Networks
ACL2024
[Paper]()

Authors: Yasamin Aali, Sardar Hamidian, Parsa Farinneya

## Abstract
The proposed system integrates a Siamese Network architecture with pre-trained language models, including BERT, RoBERTa, and the Universal Sentence Encoder. Through rigorous experimentation and analysis, we evaluate the performance of these models across multiple languages. Our findings reveal that the Universal Sentence Encoder excels in capturing semantic similarities, outperforming BERT and RoBERTa in most scenarios. Particularly notable is the USEâ€™s exceptional performance in English and Marathi. These results emphasize the importance of selecting appropriate pre-trained models based on linguistic considerations and task requirements.

## Model, Embeddings, Strategies

### Embeddings
- "Universal Sentence Encoder(USE)" for [USE]()
- "BERT" for [BERT]()
- "RoBERTa for [RoBERTa]()

## Cite This Paper
```

```
